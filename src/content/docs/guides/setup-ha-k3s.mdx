---
title: 'High Availability K3s Cluster Setup'
description: 'Complete guide for deploying highly available K3s clusters with embedded etcd, load balancing, and automatic failover for resilient container orchestration.'
pubDate: 2025-07-12
author: 'Thomas Peire'
tags:
  [
    'k3s',
    'kubernetes',
    'high-availability',
    'cluster',
    'etcd',
    'load-balancing',
    'container-orchestration',
    'homelab',
    'production',
  ]
---

import { Steps, Aside } from '@astrojs/starlight/components';

<Aside type="caution" title="Work in Progress">
  This guide is currently under development. Instructions may be incomplete or
  subject to change. Please proceed with caution and verify all steps in a test
  environment first.
</Aside>

This guide walks you through setting up a highly available K3s cluster using embedded etcd for true HA configuration. We'll use keepalived and HAProxy for load balancing and high availability with a floating IP address to eliminate single points of failure.

<Aside type="note">
  This setup uses embedded etcd rather than an external database cluster. While
  external etcd provides more flexibility, embedded etcd is sufficient for most
  homelab and small production environments while being significantly easier to
  manage.
</Aside>

## Architecture Overview

Our HA K3s setup consists of:

- **3 control plane nodes** running K3s server with embedded etcd
- **Keepalived** for floating IP management and failover
- **HAProxy** for load balancing K3s API server traffic
- **Floating IP** (`192.168.1.10`) to eliminate single points of failure

## Network Architecture Diagram

```
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚        ğŸ‘¤ Clients              â”‚
                          â”‚    kubectl, k9s, apps           â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚    ğŸŒ Floating IP (VIP)         â”‚
                          â”‚       192.168.1.10:6443         â”‚
                          â”‚    Managed by Keepalived        â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚    âš–ï¸ Load Balancer            â”‚
                          â”‚    HAProxy on control nodes     â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚   â”‚   â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                         â”‚                         â”‚
              â–¼                         â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ¢ Proxmox Node 1     â”‚ â”‚   ğŸ¢ Proxmox Node 2    â”‚ â”‚   ğŸ¢ Proxmox Node 3    â”‚
â”‚                         â”‚ â”‚                         â”‚ â”‚                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ ğŸ–¥ï¸ k3s-server-01    â”‚ â”‚ â”‚ â”‚ ğŸ–¥ï¸ k3s-server-02   â”‚ â”‚ â”‚ â”‚ ğŸ–¥ï¸ k3s-server-03    â”‚ â”‚
â”‚ â”‚ 192.168.1.11        â”‚ â”‚ â”‚ â”‚ 192.168.1.12        â”‚ â”‚ â”‚ â”‚ 192.168.1.13        â”‚ â”‚
â”‚ â”‚ Keepalived MASTER   â”‚ â”‚ â”‚ â”‚ Keepalived BACKUP   â”‚ â”‚ â”‚ â”‚ Keepalived BACKUP   â”‚ â”‚
â”‚ â”‚ HAProxy instance    â”‚ â”‚ â”‚ â”‚ HAProxy instance    â”‚ â”‚ â”‚ â”‚ HAProxy instance    â”‚ â”‚
â”‚ â”‚ Control Plane+etcd  â”‚ â”‚ â”‚ â”‚ Control Plane+etcd  â”‚ â”‚ â”‚ â”‚ Control Plane+etcd  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                         â”‚                         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ â”‚                   â”‚ â”‚
                            â–¼ â–¼                   â–¼ â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    ğŸ—„ï¸ Embedded etcd Cluster         â”‚
                    â”‚   Distributed across control plane  â”‚
                    â”‚     (servers 01, 02, 03)            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Details

| Component         | Purpose              | IP Address        | Port    | High Availability     |
| ----------------- | -------------------- | ----------------- | ------- | --------------------- |
| **Floating IP**   | Single entry point   | `192.168.1.10`    | `6443`  | Managed by Keepalived |
| **k3s-server-01** | Control plane + etcd | `192.168.1.11`    | `6443`  | Active member         |
| **k3s-server-02** | Control plane + etcd | `192.168.1.12`    | `6443`  | Active member         |
| **k3s-server-03** | Control plane + etcd | `192.168.1.13`    | `6443`  | Active member         |
| **k3s-worker-01** | Agent node           | `192.168.1.21`    | `10250` | Workload distribution |
| **k3s-worker-02** | Agent node           | `192.168.1.22`    | `10250` | Workload distribution |
| **k3s-worker-03** | Agent node           | `192.168.1.23`    | `10250` | Workload distribution |
| **Keepalived**    | IP failover          | All control nodes | VRRP    | 3-node redundancy     |
| **HAProxy**       | Load balancing       | All control nodes | `6443`  | 3-instance redundancy |

### Traffic Flow

1. **Client Connection**: Clients connect to the floating IP `192.168.1.10:6443`
2. **Keepalived**: Routes traffic to the current master node (normally k3s-server-01)
3. **HAProxy**: Load balances incoming API requests across all three K3s control plane servers
4. **K3s API**: Control plane processes requests and maintains cluster state via embedded etcd
5. **Worker Communication**: Worker nodes connect to the floating IP for resilient control plane access
6. **Workload Distribution**: Pods and services are scheduled across the worker nodes for actual workloads
7. **Failover**: If master node fails, Keepalived automatically promotes backup node

<Aside type="tip">
  This architecture ensures no single point of failure. Even if two nodes fail
  simultaneously, the cluster remains operational with the remaining node.
</Aside>

## Prerequisites

- Proxmox VE environment with VM template (see [Proxmox Ubuntu VM Template Setup Guide](/guides/proxmox-ubuntu-vm-template/))
- 3 Ubuntu VMs with at least 2 CPU cores and 4GB RAM each
- Network access between all nodes
- Basic understanding of Kubernetes concepts

## Step 1: Prepare Server VMs

<Steps>
1. **Clone VM template**
   
   Create 3 VMs from your Ubuntu template, ideally distributed across Proxmox nodes for hardware redundancy:
   
   - **VM IDs**: 301, 302, 303
   - **Names**: `k3s-server-01`, `k3s-server-02`, `k3s-server-03`
   - **Clone Mode**: Full clone
   
   <Aside type="tip">
     If your template is on a different Proxmox node, clone locally first, then migrate the VM after creation.
   </Aside>

2. **Configure hardware resources**

   Navigate to the **Hardware** tab for each VM and adjust:

   - **Memory**: 4096 MB (4GB)
   - **Processors**: 2 cores
   - **Disk**: Ensure adequate space (minimum 20GB recommended)

3. **Configure network settings**

   In the **Cloud-Init** tab, set static IP addresses:

   - **k3s-server-01**: `192.168.1.11/24`
   - **k3s-server-02**: `192.168.1.12/24`
   - **k3s-server-03**: `192.168.1.13/24`
   - **Gateway**: `192.168.1.1`

4. **Start and verify VMs**

   Start all VMs and verify:

   - Correct IP addresses are assigned
   - SSH access is working
   - Internet connectivity is available

   ```bash
   # Test connectivity to each node
   ssh root@192.168.1.11
   ssh root@192.168.1.12
   ssh root@192.168.1.13
   ```

</Steps>

## Step 2: Install and Configure Keepalived

Keepalived manages our floating IP address and provides automatic failover between nodes.

<Aside type="note">
  Perform these steps on **all three VMs**. Only the configuration will differ
  between master and backup nodes.
</Aside>

<Steps>
1. **Install keepalived**
   
   ```bash
   sudo apt update
   sudo apt install keepalived -y
   ```

2. **Create keepalived configuration**

   Create the configuration file:

   ```bash
   sudo nano /etc/keepalived/keepalived.conf
   ```

3. **Configure master node** (k3s-server-01)

   ```bash
   vrrp_instance VI_1 {
       state MASTER
       interface eth0
       virtual_router_id 55
       priority 255
       advert_int 1
       authentication {
           auth_type PASS
           auth_pass SuPeRsEcReT
       }
       virtual_ipaddress {
           192.168.1.10/24
       }
   }
   ```

4. **Configure backup nodes** (k3s-server-02 and k3s-server-03)

   For **k3s-server-02**:

   ```bash
   vrrp_instance VI_1 {
       state BACKUP
       interface eth0
       virtual_router_id 55
       priority 150
       advert_int 1
       authentication {
           auth_type PASS
           auth_pass SuPeRsEcReT
       }
       virtual_ipaddress {
           192.168.1.10/24
       }
   }
   ```

   For **k3s-server-03**:

   ```bash
   vrrp_instance VI_1 {
       state BACKUP
       interface eth0
       virtual_router_id 55
       priority 100
       advert_int 1
       authentication {
           auth_type PASS
           auth_pass SuPeRsEcReT
       }
       virtual_ipaddress {
           192.168.1.10/24
       }
   }
   ```

5. **Start and enable keepalived**

   On all nodes:

   ```bash
   sudo systemctl enable --now keepalived.service
   sudo systemctl status keepalived.service
   ```

6. **Test failover functionality**

   ```bash
   # Test basic connectivity to individual nodes
   ping -c 3 192.168.1.11
   ping -c 3 192.168.1.12
   ping -c 3 192.168.1.13

   # Test floating IP
   ping -c 3 192.168.1.10
   ```

   Test failover by stopping keepalived on the master:

   ```bash
   # On k3s-server-01 (master)
   sudo systemctl stop keepalived.service

   # The floating IP should still respond
   ping -c 3 192.168.1.10

   # Check which node took over
   sudo systemctl status keepalived.service

   # Restart the service
   sudo systemctl start keepalived.service
   ```

</Steps>

## Step 3: Install and Configure HAProxy

HAProxy will load balance the K3s API server traffic across all three nodes.

<Aside type="note">
  Install HAProxy on **all three VMs** to ensure the load balancer is also
  highly available.
</Aside>

<Steps>
1. **Install HAProxy**
   
   ```bash
   sudo apt update
   sudo apt install haproxy -y
   ```

2. **Configure HAProxy**

   Edit the HAProxy configuration:

   ```bash
   sudo nano /etc/haproxy/haproxy.cfg
   ```

   Add the following configuration at the end of the file:

   ```bash
   # K3s API Server Load Balancer
   frontend k3s_frontend
           bind *:6443
           mode tcp
           default_backend k3s_backend

   backend k3s_backend
           mode tcp
           option tcp-check
           balance roundrobin

           server k3s-server-01 192.168.1.11:6443 check
           server k3s-server-02 192.168.1.12:6443 check
           server k3s-server-03 192.168.1.13:6443 check
   ```

3. **Restart and enable HAProxy**

   ```bash
   sudo systemctl restart haproxy
   sudo systemctl enable haproxy
   sudo systemctl status haproxy
   ```

4. **Verify HAProxy configuration**

   Check for configuration errors:

   ```bash
   sudo haproxy -f /etc/haproxy/haproxy.cfg -c
   ```

</Steps>

## Step 4: Install K3s Cluster

Now we'll install K3s with embedded etcd to create our highly available cluster.

<Aside type="caution">
  The following steps are currently being refined. Ensure you have backups.
</Aside>

<Steps>
1. **Initialize the first control plane node**
   
   On **k3s-server-01**, run:
   
   ```bash
   curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server \
     --bind-address=127.0.0.1 \
     --cluster-init \
     --tls-san=192.168.1.10
   ```
   
   <Aside type="tip">
     Replace `SECRET` with a secure token of your choice. This token will be used by other nodes to join the cluster.
   </Aside>

2. **Wait for first node to be ready**

   Verify the first node is running:

   ```bash
   sudo systemctl status k3s
   sudo kubectl get nodes
   ```

3. **Join additional control plane nodes**

   On **k3s-server-02** and **k3s-server-03**, run:

   ```bash
   curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server \
     --server https://192.168.1.11:6443 \
     --bind-address=127.0.0.1 \
     --tls-san=192.168.1.10
   ```

4. **Verify cluster status**

   Check that all nodes have joined:

   ```bash
   sudo kubectl get nodes -o wide
   sudo kubectl get pods -A
   ```

</Steps>

## Step 5: Configure Agent (Worker) Nodes

Worker nodes are responsible for running your actual workloads (pods, containers). They join the cluster as agent nodes and communicate with the control plane via the floating IP for high availability.

<Aside type="note">
  Worker nodes require more resources than control plane nodes since they'll be running your applications. We'll distribute them across Proxmox nodes for hardware redundancy.
</Aside>

<Steps>

1. **Clone VM template for worker nodes**
   
   Create 3 worker VMs from the Ubuntu template, distributed across Proxmox nodes:
   
   - **VM IDs**: 311, 312, 313
   - **Names**: `k3s-worker-01`, `k3s-worker-02`, `k3s-worker-03`
   - **Clone Mode**: Full clone
   - **Distribution**: Place one worker on each Proxmox node alongside the control plane nodes

2. **Configure worker hardware resources**

   Navigate to the **Hardware** tab for each worker VM and adjust:

   - **Memory**: 24576 MB (24GB)
   - **Processors**: 2 cores
   - **Disk**: I'll keep it at 32GB for now (might change when we implement Longhorn)

   <Aside type="tip">
     Workers need more resources than control plane nodes since they run actual workloads. Adjust based on your expected application requirements.
   </Aside>

3. **Configure worker network settings**

   In the **Cloud-Init** tab, set static IP addresses:

   - **k3s-worker-01**: `192.168.1.21/24`
   - **k3s-worker-02**: `192.168.1.22/24`
   - **k3s-worker-03**: `192.168.1.23/24`
   - **Gateway**: `192.168.1.1`

4. **Start and verify worker VMs**

   Start all worker VMs and verify connectivity:

   ```bash
   # Test connectivity to each worker node
   ssh root@192.168.1.21
   ssh root@192.168.1.22
   ssh root@192.168.1.23
   ```

5. **Install K3s agent on worker nodes**

   On each worker node (**k3s-worker-01**, **k3s-worker-02**, **k3s-worker-03**), run:

   ```bash
   curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - agent \
     --server https://192.168.1.10:6443
   ```

   <Aside type="caution">
     Use the same `SECRET` token that you used when setting up the control plane cluster.
   </Aside>

   <Aside type="tip">
     Workers connect to the floating IP (`192.168.1.10`) rather than individual server IPs. This ensures high availability - if any control plane node fails, workers can still communicate with the cluster.
   </Aside>

6. **Verify cluster with worker nodes**

   From any control plane node, verify all nodes have joined:

   ```bash
   # Check all nodes are ready
   sudo kubectl get nodes -o wide
   
   # Should show something like:
   # NAME             STATUS   ROLES                  AGE   VERSION
   # k3s-server-01    Ready    control-plane,master   45m   v1.28.x+k3s1
   # k3s-server-02    Ready    control-plane,master   40m   v1.28.x+k3s1
   # k3s-server-03    Ready    control-plane,master   35m   v1.28.x+k3s1
   # k3s-worker-01    Ready    <none>                 5m    v1.28.x+k3s1
   # k3s-worker-02    Ready    <none>                 4m    v1.28.x+k3s1
   # k3s-worker-03    Ready    <none>                 3m    v1.28.x+k3s1
   
   # Check cluster info
   sudo kubectl cluster-info
   
   # Verify system pods are running
   sudo kubectl get pods -A
   ```

8. **Test workload scheduling**

   Deploy a test application to verify workers can run pods:

   ```bash
   # Create a test deployment
   sudo kubectl create deployment nginx-test --image=nginx --replicas=3
   
   # Check pod distribution across worker nodes
   sudo kubectl get pods -o wide
   
   # Clean up test deployment
   sudo kubectl delete deployment nginx-test
   ```

</Steps>

<Aside type="tip">
  Your K3s high availability cluster is now complete! You have a resilient setup with 3 control plane nodes for cluster management and 3 worker nodes for running applications, all distributed across your Proxmox infrastructure.
</Aside>

## Next Steps

- Implement Longhorn for High availability persistent storage

